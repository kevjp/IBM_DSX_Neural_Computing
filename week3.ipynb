{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Lab Sheet 3: Extracting Word Frequency Vectors with Spark\n\nThese tasks are for working in the lab session and during the week. We will use the same data as last week (19 files in './City-Data-Science/library/') and use some more RDD functions. We will apply two different approaches to create and use fixed size vectors.\n\nFirst update the repo.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "fatal: destination path 'City-Data-Science' already exists and is not an empty directory.\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb1c-2a2fc727357cc3-3b10766cfe3e/notebook/work/City-Data-Science\nFrom https://github.com/tweyde/City-Data-Science\n * branch            HEAD       -> FETCH_HEAD\nAlready up-to-date.\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/sb1c-2a2fc727357cc3-3b10766cfe3e/notebook/work\n"
                }
            ], 
            "source": "!git clone https://github.com/tweyde/City-Data-Science.git\n%cd City-Data-Science/\n!git pull https://github.com/tweyde/City-Data-Science.git\n%cd .."
        }, 
        {
            "source": "Here is code from last week that we run first, and then extend. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 106, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 106, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['h',\n 'a',\n 'd',\n 'o',\n 'o',\n 'p',\n ' ',\n 'i',\n 's',\n ' ',\n 'f',\n 'a',\n 's',\n 't',\n 'h',\n 'e',\n 'l',\n 'l',\n 'o',\n 'h',\n 'i',\n 'v',\n 'e',\n ' ',\n 'i',\n 's',\n ' ',\n 's',\n 'q',\n 'l',\n ' ',\n 'o',\n 'n',\n ' ',\n 'h',\n 'd',\n 'f',\n 's',\n 'h',\n 'e',\n 'l',\n 'l',\n 'o',\n 's',\n 'p',\n 'a',\n 'r',\n 'k',\n ' ',\n 'i',\n 's',\n ' ',\n 's',\n 'u',\n 'p',\n 'e',\n 'r',\n 'f',\n 'a',\n 's',\n 't',\n 'h',\n 'e',\n 'l',\n 'l',\n 'o',\n 's',\n 'p',\n 'a',\n 'r',\n 'k',\n ' ',\n 'i',\n 's',\n ' ',\n 'a',\n 'w',\n 'e',\n 's',\n 'o',\n 'm',\n 'e',\n 'h',\n 'e',\n 'l',\n 'l',\n 'o']"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "from pyspark import SparkContext\nsc = SparkContext.getOrCreate()\n\ntestRDD = sc.parallelize([\"hadoop is fast\", \"hive is sql on hdfs\", \"spark is superfast\", \"spark is awesome\"])\n\ntestRDD.collect()\n\n# ['hadoop is fast',\n# 'hive is sql on hdfs',\n# 'spark is superfast',\n# 'spark is awesome']\n\n\n#wc = testRDD.map(lambda line:line.split(\" \"));\nwc = testRDD.map(lambda line:line + \"hello\");\n\nwc.collect()\n\n#[['hadoop', 'is', 'fast'],\n# ['hive', 'is', 'sql', 'on', 'hdfs'],\n# ['spark', 'is', 'superfast'],\n# ['spark', 'is', 'awesome']]\n\n# ['hadoop is fasthello',\n#  'hive is sql on hdfshello',\n#  'spark is superfasthello',\n#  'spark is awesomehello']\n\n\n#fm = testRDD.flatMap(lambda line:line.split(\" \"));\nfm = testRDD.flatMap(lambda line:line + \"hello\");\nfm.collect()\n\n# Flatmap flattens each component of the inner list into separate components. If there is no inner list then each component of the list is flattened into its component parts i.e. each character)\n\n# ['hadoop',\n#  'is',\n#  'fast',\n#  'hive',\n#  'is',\n#  'sql',\n#  'on',\n#  'hdfs',\n#  'spark',\n#  'is',\n#  'superfast',\n#  'spark',\n#  'is',\n#  'awesome']\n\n\n# ['h',\n#  'a',\n#  'd',\n#  'o',\n#  'o',\n#  'p',\n#  ' ',\n#  'i',\n#  's',\n#  ' ',\n#  'f',\n#  'a',\n#  's',\n#  't',\n#  'h',\n#  'e',\n#  'l',\n#  'l',\n#  'o',\n#  'h',\n#  'i',\n#  'v',\n#  'e',\n#  ' ',\n#  'i',\n#  's',\n#  ' ',\n#  's',\n#  'q',\n#  'l',\n#  ' ',\n#  'o',\n#  'n',\n#  ' ',\n#  'h',\n#  'd',\n#  'f',\n#  's',\n#  'h',\n#  'e',\n#  'l',\n#  'l',\n#  'o',\n#  's',\n#  'p',\n#  'a',\n#  'r',\n#  'k',\n#  ' ',\n#  'i',\n#  's',\n#  ' ',\n#  's',\n#  'u',\n#  'p',\n#  'e',\n#  'r',\n#  'f',\n#  'a',\n#  's',\n#  't',\n#  'h',\n#  'e',\n#  'l',\n#  'l',\n#  'o',\n#  's',\n#  'p',\n#  'a',\n#  'r',\n#  'k',\n#  ' ',\n#  'i',\n#  's',\n#  ' ',\n#  'a',\n#  'w',\n#  'e',\n#  's',\n#  'o',\n#  'm',\n#  'e',\n#  'h',\n#  'e',\n#  'l',\n#  'l',\n#  'o']"
        }, 
        {
            "execution_count": 126, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "/gpfs/fs01/user/sb1c-2a2fc727357cc3-3b10766cfe3e/notebook/work\r\n"
                }, 
                {
                    "execution_count": 126, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('prideandpredjudice', 'the'),\n ('prideandpredjudice', 'of'),\n ('prideandpredjudice', 'pride'),\n ('prideandpredjudice', 'and'),\n ('prideandpredjudice', 'prejudice'),\n ('prideandpredjudice', 'by'),\n ('prideandpredjudice', 'jane'),\n ('prideandpredjudice', 'austen'),\n ('prideandpredjudice', 'thi'),\n ('prideandpredjudice', 'i')]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import re \n\ndef stripFinalS( word ):\n    word = word.lower() # lower case\n    if len(word) >0 and word[-1] == 's': # check for final letter\n        return word[:-1]\n    else:\n        return word\n    \ndef splitFileWords(filenameContent): # your splitting function\n    f,c = filenameContent # split the input tuple  \n    fwLst = [] # the new list for (filename,word) tuples\n    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n    for w in wLst : # iterate through the list\n        fwLst.append((f,stripFinalS(w))) # and append (f,w) to the \n    return fwLst #return a list of (f,w) tuples \n\nfrom pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\n!pwd\ndirPath = './City-Data-Science/library/' #  path\nft_RDD = sc.wholeTextFiles(dirPath) # create an RDD with wholeTextFiles\n#ft_RDD.take(1)\nfnt_RDD = ft_RDD.map(lambda ft: (re.split('[/\\.]',ft[0])[-2],ft[1])) # just take filenames, \n#                                                 # drop path and extension for readability\n\n#fnt_RDD.take(10)\n\nfw_RDD1 = fnt_RDD.flatMap(splitFileWords) # split words per file, strip final 's'\nfw_RDD1.take(10)\n# with flatmap fw_RDD1 looks like below with map each book is in a separate list so you have a list of lists structure\n# [('prideandpredjudice', ''),\n#  ('prideandpredjudice', 'the'),\n#  ('prideandpredjudice', 'project'),\n#  ('prideandpredjudice', 'gutenberg'),\n#  ('prideandpredjudice', 'ebook'),\n#  ('prideandpredjudice', 'of'),\n#  ('prideandpredjudice', 'pride'),\n#  ('prideandpredjudice', 'and'),\n#  ('prideandpredjudice', 'prejudice'),\n#  ('prideandpredjudice', 'by')]\n\nfw_RDD = fw_RDD1.filter(lambda fw: fw[1] not in ['','project','gutenberg', 'ebook']) # get rid of some unwanted words\nfw_RDD.take(10)\n# output should look like this: [('emma', 'the'), ('emma', 'of'), ('emma', 'emma')]"
        }, 
        {
            "source": "## 1) Warm-up\nLet's start with a few small tasks, to become more fluent with RDDs and lambda expressions.\n\na) Count the number of documents.\nb) Determine the number distinct words in total (the vocabulary size) using RDD.distinct(). This involves removing the fs from the (f,w) pairs and geting getting the RDD size (with RDD.count()). \nc) Get the number of words (including repeated ones) per book. \nd) Determine the number of distinct words per book. This involves determining the disting (f,w) pairs, geting a list of words per file, and getting the list size.\ne) Count the average number of occurences per word per file (words/vocabulary). Use RDD.join() to get both numbers into one RDD. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 108, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of documents:  18\nTotal vocabulary size:  23368\nTotal vocabulary size (v2):  23368\nWords per book:  [('othello', 30617), ('macbeth', 20810), ('lady_susan', 26109)]\ndefaultdict(<class 'int'>, {'macbeth': 20810, 'othello': 30617, 'prideandpredjudice': 125212, 'julius_cesar': 22959, 'lady_susan': 26109, 'midsummer': 19814, 'senseandsensibility': 123066, 'mansfield_park': 163509, 'merchant_of_venice': 24772, 'richard_III': 33785, 'king_lear': 30119, 'tempest': 27891, 'persuasion': 83680, 'northanger_abbey': 77582, 'hamlet': 34518, 'romeo_and_juliet': 29538, 'henry_V': 29900, 'emma': 163979})\nWords per book (v2):  defaultdict(<class 'int'>, {'macbeth': 20810, 'othello': 30617, 'prideandpredjudice': 125212, 'julius_cesar': 22959, 'lady_susan': 26109, 'midsummer': 19814, 'senseandsensibility': 123066, 'mansfield_park': 163509, 'merchant_of_venice': 24772, 'richard_III': 33785, 'king_lear': 30119, 'tempest': 27891, 'persuasion': 83680, 'northanger_abbey': 77582, 'hamlet': 34518, 'romeo_and_juliet': 29538, 'henry_V': 29900, 'emma': 163979})\nVocabulary per book:  [('macbeth', 3654), ('othello', 4077), ('lady_susan', 3097)]\nVocabulary per book (v2):  defaultdict(<class 'int'>, {'othello': 4077, 'mansfield_park': 7492, 'hamlet': 4447, 'prideandpredjudice': 6181, 'julius_cesar': 2834, 'lady_susan': 3097, 'king_lear': 3892, 'senseandsensibility': 6073, 'merchant_of_venice': 3686, 'richard_III': 3824, 'midsummer': 3448, 'tempest': 4727, 'macbeth': 3654, 'romeo_and_juliet': 3722, 'persuasion': 5268, 'northanger_abbey': 5510, 'henry_V': 4815, 'emma': 6667})\n"
                }
            ], 
            "source": "from operator import add\n# a) Library size\nprint(\"Number of documents: \",ft_RDD.count()) # count the number of docs\n\n# b) Vocabulary size\nw_RDD = fw_RDD.map( lambda ft: ft[1] ) # remove the file names, keep just the words\nw_RDDu = w_RDD.distinct() # keep only one unique instance of every word\nprint('Total vocabulary size: ',w_RDDu.count()) # \nprint('Total vocabulary size (v2): ',fw_RDD.values().distinct().count())\n\n\n# c) words per book\nf1_RDD = fw_RDD.map(lambda fw: (fw[0],1)) # swap and wrap (f,w) to (f,1)\nf1_RDD.take(3)\nfc_RDD = f1_RDD.reduceByKey(add) # add the 1s up\nprint('Words per book: ',fc_RDD.take(3))\nword_doc_count = fw_RDD.countByKey()\nprint(word_doc_count)\n# or alternatively (both versions should produce the same output):\nprint('Words per book (v2): ',fw_RDD.countByKey())\n\n\nfrom operator import add\nfw_RDDu = fw_RDD.distinct() # get unique (f,w) pairs - i.e. evey word only once per file. I use postfix u to mark 'unique'\nf1_RDDu = fw_RDDu.map(lambda fw: (fw[0],1)) # wrap (f,w) to (f,1)\nfcu_RDD = f1_RDDu.reduceByKey(add) # add the 1s up\nprint('Vocabulary per book: ',fcu_RDD.take(3)) \n# or, again, shorter:\nprint('Vocabulary per book (v2): ',fw_RDD.distinct().countByKey())\n"
        }, 
        {
            "execution_count": 109, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Number of documents:  18\nTotal vocabulary size:  23368\nWords per book:  [('othello', 30617), ('macbeth', 20810), ('lady_susan', 26109)]\nWords per book:  [('macbeth', 20810), ('othello', 30617), ('lady_susan', 26109)]\ndefaultdict(<class 'int'>, {'macbeth': 20810, 'othello': 30617, 'prideandpredjudice': 125212, 'julius_cesar': 22959, 'lady_susan': 26109, 'midsummer': 19814, 'senseandsensibility': 123066, 'mansfield_park': 163509, 'merchant_of_venice': 24772, 'richard_III': 33785, 'king_lear': 30119, 'tempest': 27891, 'persuasion': 83680, 'northanger_abbey': 77582, 'hamlet': 34518, 'romeo_and_juliet': 29538, 'henry_V': 29900, 'emma': 163979})\nVocabulary per book:  [('macbeth', 3654), ('othello', 4077), ('lady_susan', 3097)]\nVocabulary per book (v2):  defaultdict(<class 'int'>, {'othello': 4077, 'mansfield_park': 7492, 'hamlet': 4447, 'prideandpredjudice': 6181, 'julius_cesar': 2834, 'lady_susan': 3097, 'king_lear': 3892, 'senseandsensibility': 6073, 'merchant_of_venice': 3686, 'richard_III': 3824, 'midsummer': 3448, 'tempest': 4727, 'macbeth': 3654, 'romeo_and_juliet': 3722, 'persuasion': 5268, 'northanger_abbey': 5510, 'henry_V': 4815, 'emma': 6667})\n[('othello', (30617, 4077)), ('macbeth', (20810, 3654)), ('lady_susan', (26109, 3097))]\nAverage word occurences:  [('othello', 7.509688496443463), ('macbeth', 5.695128626163109), ('lady_susan', 8.430416532127866)]\n"
                }
            ], 
            "source": "from operator import add\n# a) Library size\nprint(\"Number of documents: \",ft_RDD.count()) # count the number of docs\n\n# b) Vocabulary size\nw_RDD = fw_RDD.map( lambda ft: ft[1] ) # remove the file names, keep just the words\nw_RDDu = w_RDD.distinct() # keep only one unique instance of every word\nprint('Total vocabulary size: ',w_RDDu.count()) # \n\n# c) words per book\nf1_RDD = fw_RDD.map(lambda fw: (fw[0],1)) # swap and wrap (f,w) to (f,1)\nfc_RDD = f1_RDD.reduceByKey(add) # add the 1s up\nprint('Words per book: ',fc_RDD.take(3))\n\n# extra task: try also to express this with one function that appeared in the lecture today\nprint('Words per book: ',fc_RDD.take(3))\nword_doc_count = fw_RDD.countByKey()\nprint(word_doc_count)\n\n\n# d) Vocabulary per book\nfrom operator import add\nfw_RDDu = fw_RDD.distinct() # get unique (f,w) pairs - i.e. evey word only once per file. I use postfix u to mark 'unique'\nf1_RDDu = fw_RDDu.map(lambda fw: (fw[0],1)) # wrap (f,w) to (f,1)\nfcu_RDD = f1_RDDu.reduceByKey(add) # add the 1s up\nprint('Vocabulary per book: ',fcu_RDD.take(3)) \n# extra task: try also replacing the map and reduce by one function \n# or, again, shorter:\nprint('Vocabulary per book (v2): ',fw_RDD.distinct().countByKey())\n\nfc_RDD.take(3)\nfcu_RDD.take(3)\n\n\n# e) Average occurences of words per book (i.e. words/vocab per book)\nf_wv_RDD = fc_RDD.join(fcu_RDD) # join the two RDDs to get (f,(w,v)) tuples\n\n# fc_RDD looks like this \n# [('macbeth', 20810), ('othello', 30617), ('lady_susan', 26109)]\n# fcu_RDD looks like this\n# [('othello', 4077), ('macbeth', 3654), ('lady_susan', 3097)]\nprint(f_wv_RDD.take(3)) \nf_awo_RDD = f_wv_RDD.map(lambda f_wv: (f_wv[0], f_wv[1][0]/f_wv[1][1] )) # this is the tricky part. \n            # Resolve nested tuples in the lambda to get (filename,words/vocab) tuples\nprint('Average word occurences: ',f_awo_RDD.take(3))\n# should look like this [('henry_V', 6.237027812370278), ('king_lear', 7.815661103979461), ('lady_susan', 8.531763947113834)]\n"
        }, 
        {
            "source": "\n## 2) Fixed vectors: Reduced vocabulary approach\n\nThe first task in this lab is to use a reduced vocabulary, only the stopwords from a list, to make sure that we have a fixed size vector. This is a common approach in stylometry. The problem is that some stopwords might not appear in some documents. We will deal with that by creating an RDD with ((f,w),0) tuples that we then merge with the ((f,w),count) RDD. \n\nStart by running the code above, then you can add 1s and use reduceByKey(add) like last week to get the counts of the words per filename.\n\nThen, please make sure that all stopwords are present by creating a new RDD that contains the keys of the fw_RDD, i.e. the filenames, using the keys() method of class RDD. Then you can use flatMap to create a [((filname,stopword),0), ...] list, using a list comprehension. The 0s should not be 1s, as we don't want add to add extra counts.\nThe RDD with ((filename,stopword),0) tuples can then be merged with fw_RDD2 using union(). Then you can count as normal.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 110, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[(('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'by'), 1), (('prideandpredjudice', 'i'), 1), (('prideandpredjudice', 'for'), 1), (('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'at'), 1), (('prideandpredjudice', 'you'), 1), (('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'at'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'by'), 1), (('prideandpredjudice', 'by'), 1), (('prideandpredjudice', 'i'), 1), (('prideandpredjudice', 'a'), 1), (('prideandpredjudice', 'a'), 1), (('prideandpredjudice', 'in'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'a'), 1), (('prideandpredjudice', 'in'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'a'), 1), (('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'a'), 1), (('prideandpredjudice', 'on'), 1), (('prideandpredjudice', 'a'), 1), (('prideandpredjudice', 'i'), 1), (('prideandpredjudice', 'in'), 1), (('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'i'), 1), (('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'you'), 1), (('prideandpredjudice', 'i'), 1), (('prideandpredjudice', 'at'), 1), (('prideandpredjudice', 'i'), 1), (('prideandpredjudice', 'for'), 1), (('prideandpredjudice', 'me'), 1), (('prideandpredjudice', 'you'), 1), (('prideandpredjudice', 'me'), 1), (('prideandpredjudice', 'i'), 1), (('prideandpredjudice', 'you'), 1), (('prideandpredjudice', 'i'), 1)]\n"
                }
            ], 
            "source": "from operator import add\n\nstopwlst = ['the','a','in','of','on','at','for','by','i','you','me'] # stopword list\nfw_RDD2 = fw_RDD.filter(lambda x: x[1] in stopwlst) # filter, keeping only stopwords\n\n# fw_RDD2 structure\n# [('prideandpredjudice', 'the'),\n#  ('prideandpredjudice', 'of'),\n#  ('prideandpredjudice', 'by')]\n\nfsw_0_RDD = fw_RDD.keys().flatMap(lambda f: [((f,sw),0) for sw in stopwlst])\n\n\n# fsw_0_RDD structure\n# [(('prideandpredjudice', 'the'), 0), (('prideandpredjudice', 'a'), 0), (('prideandpredjudice', 'in'), 0)]\n\n######## CHECK LINE BELOW\nfw_1_RDD = fw_RDD2.map(lambda fw: ((fw[0],fw[1]),1))  #<<< change (f,w) to ((f,w),1)\n\n\nfw_10_RDD = fw_1_RDD.union(fsw_0_RDD) #<<< create the union on the two RDDs\nprint(fw_10_RDD.take(50))"
        }, 
        {
            "execution_count": 111, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[(('prideandpredjudice', 'the'), 0), (('prideandpredjudice', 'a'), 0), (('prideandpredjudice', 'in'), 0)]\n[(('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'by'), 1)]\n[(('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'by'), 1), (('prideandpredjudice', 'i'), 1), (('prideandpredjudice', 'for'), 1), (('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'at'), 1), (('prideandpredjudice', 'you'), 1), (('prideandpredjudice', 'the'), 1)]\n[(('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'by'), 1)]\n[(('mansfield_park', 'the'), 6382), (('midsummer', 'in'), 265), (('prideandpredjudice', 'in'), 1937)]\n"
                }
            ], 
            "source": "from operator import add\n\nstopwlst = ['the','a','in','of','on','at','for','by','i','you','me'] # stopword list\nfw_RDD2 = fw_RDD.filter(lambda x: x[1] in stopwlst) # filter, keeping only stopwords\n\n# fw_RDD2 structure\n# [('prideandpredjudice', 'the'),\n#  ('prideandpredjudice', 'of'),\n#  ('prideandpredjudice', 'by')]\n\nfsw_0_RDD = fw_RDD.keys().flatMap(lambda f: [((f,sw),0) for sw in stopwlst])\nprint(fsw_0_RDD.take(3))\n\n# fsw_0_RDD structure\n# [(('prideandpredjudice', 'the'), 0), (('prideandpredjudice', 'a'), 0), (('prideandpredjudice', 'in'), 0)]\n\n######## CHECK LINE BELOW\nfw_1_RDD = fw_RDD2.map(lambda fw: ((fw[0],fw[1]),1))  #<<< change (f,w) to ((f,w),1)\nprint(fw_1_RDD.take(3))\nfw_1_RDD = fw_RDD2.map(lambda fw: (fw,1))  #<<< change (f,w) to ((f,w),1)\nprint(fw_1_RDD.take(10))\n# # fw_1_RDD structure\n# # [(('prideandpredjudice', 'the'), 1), (('prideandpredjudice', 'of'), 1), (('prideandpredjudice', 'by'), 1)]\n\nfw_10_RDD = fw_1_RDD.union(fsw_0_RDD) #<<< create the union on the two RDDs\nprint(fw_10_RDD.take(3))\n\nfw_c_RDD = fw_10_RDD.reduceByKey(add) #<<< count the words\nprint(fw_c_RDD.take(3))\n# # output should look like this:\n# #[(('emma', 'the'), 0), (('emma', 'a'), 0), (('emma', 'in'), 0)]\n# #[(('emma', 'the'), 1), (('emma', 'of'), 1), (('emma', 'by'), 1)]\n# #[(('emma', 'the'), 1), (('emma', 'of'), 1), (('emma', 'by'), 1)]\n# #[(('emma', 'the'), 5380), (('emma', 'by'), 591), (('emma', 'you'), 2068)]"
        }, 
        {
            "source": "## 3) Creating sorted lists\n\nAs a next step, map the `((filename,word),count)` to `( filename, [ (word, count) ])` using the function `reGrpLst` to regroup and create a list. \n\nThen sort the [(word,count),...] lists in the values (i.e. 2nd part of the tuple) with the the words as keys. Have a [look at the Python docs](https://docs.python.org/3.5/library/functions.html?highlight=sorted#sorted) for how to do this. Hint: use a lambda that extracts the words as the key, e.g. `sorted(f_wdL[1], key = lambda wc: ... )`.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 112, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[('othello', [('a', 668), ('at', 78), ('by', 131), ('for', 270), ('i', 1222), ('in', 349), ('me', 281), ('of', 542), ('on', 132), ('the', 820), ('you', 552)]), ('lady_susan', [('a', 611), ('at', 161), ('by', 152), ('for', 262), ('i', 1106), ('in', 402), ('me', 200), ('of', 787), ('on', 140), ('the', 784), ('you', 353)]), ('persuasion', [('a', 2404), ('at', 533), ('by', 418), ('for', 708), ('i', 1522), ('in', 1389), ('me', 188), ('of', 2570), ('on', 396), ('the', 3329), ('you', 628)])]\n"
                }, 
                {
                    "execution_count": 112, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('othello',\n  [668.0,\n   78.0,\n   131.0,\n   270.0,\n   1222.0,\n   349.0,\n   281.0,\n   542.0,\n   132.0,\n   820.0,\n   552.0]),\n ('lady_susan',\n  [611.0,\n   161.0,\n   152.0,\n   262.0,\n   1106.0,\n   402.0,\n   200.0,\n   787.0,\n   140.0,\n   784.0,\n   353.0]),\n ('persuasion',\n  [2404.0,\n   533.0,\n   418.0,\n   708.0,\n   1522.0,\n   1389.0,\n   188.0,\n   2570.0,\n   396.0,\n   3329.0,\n   628.0])]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "def reGrpLst(fw_c): # we get a nested tuple\n    fw,c = fw_c # split the outer tuple\n    f,w = fw     # split the inner tuple\n    return (f,[(w,c)]) # return (f,[(w,c)]) structure. Can be used verbatim, if your variable names match.\n\n\nf_wcL_RDD = fw_c_RDD.map(reGrpLst) # apply reGrpLst\nf_wcL2_RDD = f_wcL_RDD.reduceByKey(add) # create [(w,c), ... ,(w,c)] lists per file \nf_wcL2_RDD.take(5)\nf_wcLsort_RDD = f_wcL2_RDD.map(lambda f_wcL: (f_wcL[0], sorted(f_wcL[1],key=lambda x: x[0]))) #<<< sort the word count lists by word\nprint(f_wcLsort_RDD.take(3)) \nf_wVec_RDD = f_wcLsort_RDD.map(lambda f_wc: (f_wc[0],[float(c) for w,c in f_wc[1]])) # remove the words from the wc pairs and convert the numbers to floats\nf_wVec_RDD.take(3)\n# output:\n# [('lady_susan', [('a', 611), ('at', 161), ('by', 152), ('for', 262), ('i', 1106), ('in', 402), ('me', 200), ('of', 787), ..."
        }, 
        {
            "source": "## 4) Clustering\n\nNow we have feature vectors of fixed size, we can use KMeans as provided by Spark.\n\nThe files in our library are by two authors. After clustering, check if the cluters reflect authorship:\n\nWILLIAM SHAKESPEARE: \nmerchant_of_venice, \nrichard_III, \nmidsummer,\ntempest,\nromeoandjuliet,\nothello,\nhenry_V,\nmacbeth,\nking_lear,\njulius_cesar,\nhamlet\n\nJANE AUSTEN\nmansfield_park,\nemma,\nnorthanger_abbey,\nlady_susan,\npersuasion,\nprideandpredjudice,\nsenseandsensibility", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 113, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "f_wVec_RDD.take(2):  [('othello', [668.0, 78.0, 131.0, 270.0, 1222.0, 349.0, 281.0, 542.0, 132.0, 820.0, 552.0])]\n[[668.0, 78.0, 131.0, 270.0, 1222.0, 349.0, 281.0, 542.0, 132.0, 820.0, 552.0], [611.0, 161.0, 152.0, 262.0, 1106.0, 402.0, 200.0, 787.0, 140.0, 784.0, 353.0], [2404.0, 533.0, 418.0, 708.0, 1522.0, 1389.0, 188.0, 2570.0, 396.0, 3329.0, 628.0]]\n[array([ 3452.        ,   802.        ,   604.5       ,  1125.5       ,\n        2816.        ,  1906.33333333,   379.        ,  3609.33333333,\n         616.16666667,  4507.5       ,  1321.83333333]), array([ 591.91666667,   86.75      ,  134.91666667,  236.83333333,\n        929.25      ,  342.58333333,  217.08333333,  578.        ,\n        108.08333333,  895.91666667,  423.08333333])]\n('othello', 1)\n('lady_susan', 1)\n('persuasion', 0)\n('macbeth', 1)\n('merchant_of_venice', 1)\n('mansfield_park', 0)\n('julius_cesar', 1)\n('emma', 0)\n('tempest', 1)\n('richard_III', 1)\n('midsummer', 1)\n('northanger_abbey', 0)\n('senseandsensibility', 0)\n('prideandpredjudice', 0)\n('romeo_and_juliet', 1)\n('king_lear', 1)\n('henry_V', 1)\n('hamlet', 1)\nWithin Set Sum of Squared Error = 15118.000047182639\n17\n"
                }
            ], 
            "source": "\nfrom math import sqrt\n\nfrom pyspark.mllib.clustering import KMeans #, KMeansModel\n\nprint('f_wVec_RDD.take(2): ', f_wVec_RDD.take(1))\nwVec_RDD = f_wVec_RDD.map(lambda f_wcl: f_wcl[1]) # strip the filenames, keep only the vectors\nprint(wVec_RDD.take(3))\n\n# Build the model (cluster the data)\nclusterModel = KMeans.train(wVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\nprint (clusterModel.centers)\n\n# Assign the filenames to the clusters\nfc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\nfor s in fc_RDD.collect():\n    print(s)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef error(point):\n    center = clusterModel.centers[clusterModel.predict(point)] # coordinate of center of each of the 2 clusters\n    return sqrt(sum([x**2 for x in (point - center)])) # returns sum of the squares between point and its cluster center\n\nWSSSE = wVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n# now check if the clusters match the authors\n# output:\n#('lady_susan', 1)\n#('macbeth', 1)\n#('merchant_of_venice', 1)\n#('othello', 1)\n#('persuasion', 0)\n#('emma', 0)\n\n\ntestRDD = sc.parallelize([1, 2, 4, 10])\ntestRDD1 = testRDD.reduce(lambda x,y: x + y)\nprint(testRDD1)"
        }, 
        {
            "source": "## 4) Alternative approach: feature hashing\n\nInstead of the previous appraoch, we now use feature hashing, as done last week.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 114, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[('othello', [3210, 3107, 2080, 3437, 3237, 3216, 2510, 4440, 1962, 3418]), ('macbeth', [1964, 2024, 1877, 2148, 2010, 2234, 1610, 3167, 1339, 2437]), ('lady_susan', [3448, 2639, 1874, 2737, 2847, 2894, 1994, 3851, 1158, 2667])]\n"
                }
            ], 
            "source": "def hashing_vectorizer(word_count_list, N):\n    v = [0] * N  # create fixed size vector of 0s\n    for word_count in word_count_list: \n        word, count =  word_count    # unpack tuple\n        h = hash(word)              # get hash value\n        v[h % N] = v[h % N] + count # add count\n    return v # return hashed word vector\n\nfrom operator import add\n\nN = 10\n\n# we use fw_RDD from the beginning with all the words, not just stopwords\nfw_1_RDD = fw_RDD.map(lambda fw: (fw, 1))  #<<< change (f,w) to ((f,w),1)\n#fw_1_RDD.take(3)                \nfw_c_RDD = fw_1_RDD.reduceByKey(add) #as above\nf_wcL_RDD = fw_c_RDD.map(reGrpLst) #as above\nf_wcL_RDD.take(10)\n\nf_wcL2_RDD = f_wcL_RDD.reduceByKey(add) #create [(w,c), ... ,(w,c)] lists per file \nf_wcL2_RDD.take(10)\nf_wVec_RDD = f_wcL2_RDD.map(lambda f_wcl: (f_wcl[0], hashing_vectorizer(f_wcl[1], N))) # apply the hashing_vectorizer to the word-count list\nprint(f_wVec_RDD.take(3))\n# output:\n# [('henry_V', [2277, 2293, 1182, 1792, 2058, 1550, 787, 1821, 814, 1916, 902, 752, 1249, 1022, 888, 1702, 1357, 2886, 1007, 1645]), ('king_lear', [2149, 2217, 1010, 2167, 2331, 1372, 726, 1682, 747, 1623, 1470, 889, 1248, 1371, 1062, 1472, 1510, 2456, 1364, 1253]), ('lady_susan', [2015, 1850, 823, 1828, 2099, 1658, 704, 1656, 588, 1319, 1433, 789, 1051, 909, 748, 1236, 1290, 2195, 570, 1348])]"
        }, 
        {
            "execution_count": 115, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "f_wVec_RDD.take(2):  [('macbeth', [1964, 2024, 1877, 2148, 2010, 2234, 1610, 3167, 1339, 2437])]\n[[3210, 3107, 2080, 3437, 3237, 3216, 2510, 4440, 1962, 3418], [1964, 2024, 1877, 2148, 2010, 2234, 1610, 3167, 1339, 2437], [3448, 2639, 1874, 2737, 2847, 2894, 1994, 3851, 1158, 2667], [3619, 3106, 2258, 3538, 3393, 2844, 2236, 4138, 2111, 2876], [17073, 12753, 7663, 11839, 10487, 13163, 9662, 20753, 5831, 13842], [2307, 2412, 1824, 2844, 2726, 2764, 1921, 3863, 1401, 2710], [2039, 2084, 1604, 2005, 1986, 2244, 1637, 2750, 1253, 2212], [17312, 14009, 7456, 11714, 11005, 13677, 9499, 21236, 5907, 13397], [11344, 8607, 5039, 7593, 6855, 9336, 6130, 14833, 4520, 9423], [10493, 8084, 5098, 7615, 6689, 8377, 5797, 12660, 3987, 8782], [3195, 3043, 2241, 3201, 3040, 3009, 2469, 3814, 2354, 3172], [3179, 3045, 2431, 2814, 2946, 3252, 2144, 4707, 1821, 3561], [22749, 17943, 9771, 16800, 14250, 16454, 12948, 27140, 7448, 18006], [4228, 2689, 2212, 3057, 2638, 2747, 2266, 3548, 1881, 2625], [3753, 3415, 2405, 3823, 3166, 3426, 2864, 5295, 2537, 3101], [2136, 2578, 1622, 2744, 2472, 2432, 2037, 3129, 1816, 1993], [3873, 3580, 2721, 4039, 3680, 3396, 2704, 5027, 2163, 3335], [23010, 18465, 8917, 16202, 14805, 15913, 13368, 25930, 8212, 19157]]\n('othello', 0)\n('macbeth', 0)\n('lady_susan', 0)\n('king_lear', 0)\n('senseandsensibility', 1)\n('merchant_of_venice', 0)\n('midsummer', 0)\n('prideandpredjudice', 1)\n('persuasion', 0)\n('northanger_abbey', 0)\n('romeo_and_juliet', 0)\n('henry_V', 0)\n('mansfield_park', 1)\n('tempest', 0)\n('richard_III', 0)\n('julius_cesar', 0)\n('hamlet', 0)\n('emma', 1)\nWithin Set Sum of Squared Error = 90883.15633407317\n"
                }
            ], 
            "source": "from math import sqrt\n\nfrom pyspark.mllib.clustering import KMeans #, KMeansModel\n\nprint('f_wVec_RDD.take(2): ', f_wVec_RDD.take(1))\nwVec_RDD = f_wVec_RDD.map(lambda f_wcl: f_wcl[1]) # strip the filenames\nprint(wVec_RDD.collect())\n\n# Build the model (cluster the data)\nclusterModel = KMeans.train(wVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n\n# Assign the files to the clusters\nfc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1]))) \nfor s in fc_RDD.collect():\n    print(s)\n    \n# resusing 'error' function from abov\nWSSSE = wVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
        }, 
        {
            "source": "## 5) Neutralising document length: Normalised vectors\n\n'Lady Susan' ends up reliably in the wrong cluster. A possible explanation is that it is shorter than the other Austen works. Try normalising the word counts, i.e. by dividing by their sum. That takes away the effect of length. What is the effect on the clustering?\n    \nYou can use a list comprehension for the normalisation.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": 118, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Normalised vectors:  [[0.09437770302739068, 0.0972609322441134, 0.09019702066314272, 0.10321960595867372, 0.09658817876021143, 0.10735223450264296, 0.07736665064872657, 0.1521864488226814, 0.06434406535319558, 0.11710716001922153], [0.10484371427638241, 0.10147957017343306, 0.0679361139236372, 0.11225789594016396, 0.10572557729366039, 0.10503968383577751, 0.08198059901361988, 0.1450174739523794, 0.06408204592220008, 0.11163732566874612], [0.1320617411620514, 0.1010762572293079, 0.07177601593320311, 0.10482975219273048, 0.10904285878432725, 0.1108430043280095, 0.07637213221494504, 0.14749703167490139, 0.044352522118809606, 0.10214868436171436]]\nNormalised vectors:  [('othello', [3210, 3107, 2080, 3437, 3237, 3216, 2510, 4440, 1962, 3418]), ('macbeth', [1964, 2024, 1877, 2148, 2010, 2234, 1610, 3167, 1339, 2437]), ('lady_susan', [3448, 2639, 1874, 2737, 2847, 2894, 1994, 3851, 1158, 2667])]\n('othello', 0)\n('macbeth', 0)\n('lady_susan', 0)\n('king_lear', 0)\n('senseandsensibility', 0)\n('merchant_of_venice', 0)\n('midsummer', 0)\n('prideandpredjudice', 0)\n('persuasion', 0)\n('northanger_abbey', 0)\n('romeo_and_juliet', 0)\n('henry_V', 0)\n('mansfield_park', 0)\n('tempest', 0)\n('richard_III', 0)\n('julius_cesar', 0)\n('hamlet', 0)\n('emma', 0)\n"
                }
            ], 
            "source": "from pyspark.mllib.clustering import KMeans #, KMeansModel\nnwVec_RDD = wVec_RDD.map(lambda v: ([count/sum(v) for count in v])) # provide a list comprehension that \n                            #normalises the values by dividing by the sum over the list\n# nfwVec_RDD = f_wVec_RDD.map(lambda fc: (fc[0], [count/sum(fc[1]) for count in fc[1]]))    \n    \nprint(\"Normalised vectors: \",nwVec_RDD.take(3))\nprint(\"Normalised vectors: \",f_wVec_RDD.take(3))\n\n# print(\"Normalised vectors: \",nfwVec_RDD.take(3)) \n\n\n# Build the model (cluster the data)\nclusterModel = KMeans.train(nwVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n\n# Assign the files to the clusters\n# fc_RDD = nfwVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\n# fc_RDD.take(3)\n\nfc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\nf_wVec_RDD.take(3)\nfor s in fc_RDD.collect():\n    print(s)\n# output\n# Normalised vectors:  [[0.07615384615384616, 0.07668896321070234, 0.03953177257525083, 0.05993311036789298, \n# ...                       \n# ('henry_V', 0)\n# ('king_lear', 0)\n#('lady_susan', 1)\n# .."
        }, 
        {
            "source": "## 6) Building an index\n\nStarting from the fw_RDD we now start building the index and calculating the IDF values. Since we have the TF values alread, we only need to keep the unique filenames per word using [RDD.distinct()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.distinct).  \nThen we create a list of filenames. The length of the list is the document frequency DF per word.\nFrom the DF value we can calculate the IDF value as log(18/DF) ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 125, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[('iwi', ['merchant_of_venice', 'richard_III']), ('calme', ['othello']), ('circle', ['northanger_abbey', 'king_lear', 'senseandsensibility', 'persuasion', 'romeo_and_juliet', 'henry_V', 'prideandpredjudice', 'tempest', 'mansfield_park', 'emma'])]\nDF:  [('iwi', 3), ('matrimonial', 11), ('circle', 6)]\nIDF:  [('iwi', 1.791759469228055), ('calme', 1.2809338454620642), ('circle', 1.0986122886681098)]\n"
                }
            ], 
            "source": "from operator import add\nfrom math import log\nfw_RDD.take(3)\nfwu_RDD = fw_RDD.distinct() # get unique file/word pairs\n\nwfl_RDD = fwu_RDD.map(lambda fw: (fw[1],[fw[0]])) # create (w,[f]) tuples \nwfL_RDD = wfl_RDD.reduceByKey(add) # concatenate the lists with 'add'\nprint(wfL_RDD.take(3))\n\nwdf_RDD = wfL_RDD.map(lambda wfl: (wfl[0],len(wfl[0]))) # get the DF replacing the file list with its lenght\nprint(\"DF: \",wdf_RDD.take(3))\nwidf_RDD = wdf_RDD.map(lambda wdf: (wdf[0],log(18/wdf[1]))) # get he IDF by replacing DF with log(18/DF)\nprint(\"IDF: \",widf_RDD.take(3))\n# ouptut:\n# [('of', ['henry_V', 'king_lear', 'lady_susan', 'macbeth', 'merchant_of_venice', 'midsummer', 'northanger_abbey', \n# DF:  [('of', 18), ('shakespeare', 15), ('henry', 9)]\n# IDF:  [('of', 0.0), ('shakespeare', 0.1823215567939546), ('henry', 0.6931471805599453)]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}